# Chapter 3 of NLP with Transformers

A note on the assigned reading. *The Illustrated Transformer* is widely regarded as one of the best introductions to the transformer architecture. So if you only have time to read it or Chapter 3, read *The Illustrated Transformer*. *NLP with Transformers* Chapter 3 has code examples, so do try and read both.

## Assignment

Optionally watch [*The Narrated Transformer*](https://www.youtube.com/watch?v=-QH8fRhqFHM) for an overview

Then read both [*The Illustrated Transformer*](http://jalammar.github.io/illustrated-transformer) and Chapter 3 of *NLP with Transformers*

## Additional Materials

Informative:
 - [*Attention is All You Need*](https://arxiv.org/abs/1706.03762>)
 - [*What Are Word and Sentence Embeddings?*](https://txt.cohere.ai/sentence-word-embeddings)

Introductory:
 - [*Deep Learning Foundations to Stable Diffusion* section on Attention](https://youtu.be/DH5bp6zTPB4?t=2997)
 - [HF Course Transformer overview](https://huggingface.co/learn/nlp-course/chapter1/4)

## Go even deeper

- Karpathy's [*Let's Build GPT from scratch, in code, spelled out*](https://www.youtube.com/watch?v=kCc8FmEb1nY) along with the [*minGPT* repository](https://github.com/karpathy/minGPT)
- [*The Transformer Family Version 2.0*](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2)
- [*Transformers from Scratch*](https://e2eml.school/transformers.html)
- Raschka's [*Self-attention and transformer networks* lectures](https://sebastianraschka.com/blog/2021/dl-course.html#l19-self-attention-and-transformer-networks)

Additional Materials and Go even deeper are all optional resources to pursue at your own pace. I haven't gone over them all myself, but all look useful.

If you are familiar with Transformers, perhaps because you worked through fastai part 2, I'd recommend reading *Attention is All You Need*. It's pretty accessible, as far as papers go.